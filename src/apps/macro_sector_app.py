# src/apps/macro_sector_app.py
# ======================================================================================
# Tableau de bord macro ‚Üí secteurs (version lisible, sans abr√©viations, tout visible)
# - AUCUNE action UI n‚Äôest ex√©cut√©e √† l‚Äôimport : tout est encapsul√© dans render_macro()
# - Libell√©s d√©velopp√©s (pas d‚Äôabr√©viations opaques)
# - Pas d‚Äôexpanders : tout est affich√© afin de rep√©rer facilement les probl√®mes
# ======================================================================================

try:
    from macro_firecrawl import get_macro_data_firecrawl  # optionnel
except Exception:
    def get_macro_data_firecrawl(*args, **kwargs):
        return None  # stub si le module n'est pas dispo

import os
import time
from datetime import datetime, timedelta, date, timezone
import warnings
from typing import Optional, Union
import io
import logging
from functools import wraps
from collections import OrderedDict

import numpy as np
import pandas as pd
import requests
import yfinance as yf
import plotly.graph_objects as go
from utils import get_st, warn_once
from core_runtime import (SESSION, df_fingerprint, write_entry, with_span,
                          log, get_trace_id, new_trace_id, set_trace_id, get_dataset_log_latest)

st = get_st()

# ---------- √âviter les avertissements courants ----------
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=pd.errors.PerformanceWarning)
warnings.filterwarnings("ignore", category=pd.errors.SettingWithCopyWarning)

# ================== LOGGING (en m√©moire pour l‚Äô√©cran) ==================
if "logbuf" not in st.session_state:
    st.session_state.logbuf = []

def _log(level: str, msg: str):
    ts = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S")
    line = f"[{ts} UTC] {level}: {msg}"
    st.session_state.logbuf.append(line)
    try:
        logging.getLogger("macroapp").log(getattr(logging, level, logging.INFO), msg)
    except Exception:
        pass

def log_info(msg):  _log("INFO", msg)
def log_warn(msg):  _log("WARNING", msg)
def log_error(msg): _log("ERROR", msg)

def tlog(name=None):
    """D√©corateur de timing + logging sur les fonctions de fetch/cache."""
    def deco(fn):
        @wraps(fn)
        def inner(*args, **kwargs):
            t0 = time.time()
            try:
                res = fn(*args, **kwargs)
                dur = (time.time() - t0) * 1000
                shape = None
                if isinstance(res, pd.DataFrame):
                    shape = f"{res.shape[0]}x{res.shape[1]}"
                elif isinstance(res, pd.Series):
                    shape = f"{res.shape[0]}"
                log_info(f"{name or fn.__name__} OK en {dur:.0f} ms, shape={shape}")
                return res
            except Exception as e:
                dur = (time.time() - t0) * 1000
                log_error(f"{name or fn.__name__} ERREUR en {dur:.0f} ms: {e}")
                raise
        return inner
    return deco

def profile_df(df: Optional[Union[pd.DataFrame, pd.Series]], name: str):
    out = {"Nom": name, "Type": type(df).__name__, "Lignes": 0, "Colonnes": 0,
           "D√©but": None, "Fin": None, "Fr√©quence inf√©r√©e": None, "Valeurs manquantes (m√©diane, %)": None}
    if df is None:
        return out
    if isinstance(df, pd.Series):
        df = df.to_frame()
    if not isinstance(df, pd.DataFrame) or df.empty:
        return out
    out["Lignes"], out["Colonnes"] = df.shape
    try:
        out["D√©but"] = str(df.index.min())
        out["Fin"]   = str(df.index.max())
    except Exception:
        pass
    try:
        out["Fr√©quence inf√©r√©e"] = pd.infer_freq(df.index)
    except Exception:
        out["Fr√©quence inf√©r√©e"] = None
    try:
        na_pct = (1 - df.notna().sum()/len(df)) * 100
        out["Valeurs manquantes (m√©diane, %)"] = float(na_pct.median().round(2))
    except Exception:
        pass
    return out

# ================== S√âRIES MACRO (FRED) ==================
FRED_SERIES = {
    # Inflation / anticipations
    "CPIAUCSL": "Indice des prix √† la consommation (niveau, base 1982-84=100)",
    "T10YIE":   "Inflation implicite √† 10 ans (breakeven, attentes de march√©)",
    # Activit√©
    "INDPRO":   "Indice de production industrielle",
    "GDPC1":    "Produit int√©rieur brut r√©el (trimestriel)",
    # Emploi
    "UNRATE":   "Taux de ch√¥mage",
    "PAYEMS":   "Emplois non agricoles (niveau)",
    # Taux d‚Äôint√©r√™t & courbe
    "DGS10":    "Taux du Tr√©sor US √† 10 ans",
    "DGS2":     "Taux du Tr√©sor US √† 2 ans",
    # Dollar
    "DTWEXBGS": "Indice large du dollar am√©ricain (taux de change pond√©r√©)",
    # Conditions financi√®res / cr√©dit
    "NFCI":       "Indice des conditions financi√®res (Fed de Chicago)",
    "BAMLC0A0CM": "√âcart de cr√©dit obligations d‚Äôentreprises (investment grade)",
    "BAMLH0A0HYM2": "√âcart de cr√©dit obligations √† haut rendement",
    # R√©cessions (indicateur binaire)
    "USREC":    "Indicateur de r√©cession (NBER, 1 = en r√©cession)",
}

SECTOR_ETFS = ["XLB","XLE","XLF","XLV","XLK","XLI","XLY","XLP","XLRE","XLU"]
EXTRA_ETFS  = ["GDX"]  # Mines d'or (optionnel)

# Sensibilit√©s par th√®me (positif = th√®me favorable au secteur)
DEFAULT_SENS = pd.DataFrame({
    "Inflation":{"XLK":-1,"XLF":1,"XLE":2,"XLB":2,"XLV":0,"XLY":-1,"XLP":0,"XLI":1,"XLRE":-1,"XLU":1,"GDX":2},
    "Croissance":   {"XLK": 2,"XLF":1,"XLE":0,"XLB":1,"XLV":0,"XLY": 2,"XLP":0,"XLI":1,"XLRE": 0,"XLU":-1,"GDX":0},
    "Taux d‚Äôint√©r√™t":    {"XLK":-2,"XLF":2,"XLE":1,"XLB":0,"XLV":0,"XLY":-1,"XLP":0,"XLI":0,"XLRE":-1,"XLU":1,"GDX":1},
    "Dollar am√©ricain":      {"XLK":-1,"XLF":0,"XLE":-1,"XLB":-1,"XLV":0,"XLY":0,"XLP":0,"XLI":-1,"XLRE":-1,"XLU":0,"GDX":-1},
    "March√© du travail":     {"XLK": 1,"XLF":1,"XLE":0,"XLB":1,"XLV":0,"XLY": 1,"XLI":1,"XLRE": 0,"XLU":-1,"GDX":0},
}).fillna(0)

# ================== HELPERS ANALYTIQUES ==================
def zscore(series, win=24):
    s = series.dropna()
    if len(s) < win + 2:
        return pd.Series(index=series.index, dtype=float)
    mu = s.rolling(win).mean()
    sd = s.rolling(win).std()
    return ((s - mu) / (sd.replace(0, np.nan))).reindex(series.index)

def normalize_fred_key(k: Optional[str]) -> Optional[str]:
    """Valide une cl√© FRED (32 caract√®res alphanum√©riques en minuscules)."""
    if not k or not isinstance(k, str):
        return None
    k = k.strip()
    if len(k) == 32 and k.isalnum() and k == k.lower():
        return k
    return None

def load_fred_series(series_id: str, fred_key: Optional[str] = None, start: Optional[str] = None) -> pd.DataFrame:
    """
    Charge une s√©rie FRED robuste :
      1) API JSON officielle (si cl√© valide)
      2) Fallback CSV fredgraph.csv (sans cl√©)
    """
    vkey = normalize_fred_key(fred_key)

    # 1) API JSON
    if vkey:
        try:
            params = {"series_id": series_id, "api_key": vkey, "file_type": "json"}
            if start:
                params["observation_start"] = pd.to_datetime(start).strftime("%Y-%m-%d")
            r = requests.get("https://api.stlouisfed.org/fred/series/observations",
                             params=params, timeout=30)
            r.raise_for_status()
            js = r.json()
            if "observations" in js:
                df = pd.DataFrame(js["observations"])
                if not df.empty and {"date", "value"}.issubset(df.columns):
                    df["date"] = pd.to_datetime(df["date"], errors="coerce")
                    df["value"] = pd.to_numeric(df["value"].replace(".", np.nan), errors="coerce")
                    df = df.set_index("date")[["value"]].rename(columns={"value": series_id}).sort_index()
                    return df
        except Exception as e:
            log_warn(f"API FRED JSON en √©chec pour {series_id}: {e}; essai CSV")

    # 2) CSV fredgraph
    try:
        url = f"https://fred.stlouisfed.org/graph/fredgraph.csv?id={series_id}"
        resp = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=30)
        resp.raise_for_status()
        text = resp.text.strip()
        if text[:1] in ("<", "{"):
            raise RuntimeError("FRED a renvoy√© un contenu non-CSV")
        df = pd.read_csv(io.StringIO(text))
        # colonne date
        date_col = None
        for cand in ("DATE", "date", "observation_date"):
            if cand in df.columns:
                date_col = cand; break
        if date_col is None:
            raise KeyError("CSV sans colonne DATE")
        df[date_col] = pd.to_datetime(df[date_col], errors="coerce")
        df = df.set_index(date_col)
        # colonne valeur
        if series_id in df.columns:
            val_col = series_id
        elif "VALUE" in df.columns:
            val_col = "VALUE"
        elif "value" in df.columns:
            val_col = "value"
        else:
            candidates = [c for c in df.columns if c.lower() not in ("date", "observation_date")]
            if len(candidates) == 1:
                val_col = candidates[0]
            else:
                raise KeyError(f"Impossible d‚Äôidentifier la colonne valeur pour {series_id}")
        df[val_col] = pd.to_numeric(df[val_col].replace(".", np.nan), errors="coerce")
        out = df[[val_col]].rename(columns={val_col: series_id}).sort_index()
        if start:
            out = out[out.index >= pd.to_datetime(start)]
        return out
    except Exception as e:
        log_warn(f"FRED CSV en √©chec pour {series_id}: {e}")
        return pd.DataFrame(columns=[series_id])

def get_asset_price(symbol, start="2010-01-01"):
    t = yf.Ticker(symbol)
    hist = t.history(start=start, auto_adjust=True)
    s = hist["Close"].rename(symbol)
    if getattr(s.index, "tz", None) is not None:
        s.index = s.index.tz_localize(None)
    return s

def get_multi_yf(symbols, start="2010-01-01"):
    data = {}
    for s in symbols:
        try:
            data[s] = get_asset_price(s, start=start)
            time.sleep(0.25)
        except Exception as e:
            log_warn(f"yfinance en √©chec pour {s}: {e}")
    df = pd.DataFrame(data)
    if getattr(df.index, "tz", None) is not None:
        df.index = df.index.tz_localize(None)
    return df

def robust_minmax(row):
    vals = row.values.astype(float)
    finite_mask = np.isfinite(vals)
    if finite_mask.sum() < 2:
        return pd.Series(0.0, index=row.index)
    finite_vals = vals[finite_mask]
    q1, q3 = np.nanpercentile(finite_vals, [25, 75])
    iqr = q3 - q1
    vmin = np.nanmin(finite_vals)
    vmax = np.nanmax(finite_vals)
    lo = vmin if not np.isfinite(iqr) else min(vmin, q1 - 1.5 * iqr)
    hi = vmax if not np.isfinite(iqr) else max(vmax, q3 + 1.5 * iqr)
    if not np.isfinite(hi - lo) or (hi - lo) < 1e-9:
        return pd.Series(0.0, index=row.index)
    return -5 + 10 * (row - lo) / (hi - lo)

def smooth_scores(df, ema=3):
    return df.ewm(span=ema, adjust=False).mean()

def hysteresis_picks(score_df, k=3, margin=0.15):
    ranks = score_df.rank(axis=1, ascending=False, method="first")
    base = (ranks <= k).astype(float)
    prev = None
    out = base.copy()
    for t in score_df.index:
        row = score_df.loc[t]
        b = base.loc[t]
        if prev is None:
            prev = b
            out.loc[t] = b
            continue
        kth  = row.nlargest(k).min()
        kth1 = row.nlargest(min(k+1, len(row))).min()
        keep  = (prev == 1) & (row >= (kth1 - margin))
        enter = (prev == 0) & (row >= (kth + margin))
        out.loc[t] = (keep | enter).astype(float)
        prev = out.loc[t]
    w = out.div(out.sum(axis=1).replace(0, np.nan), axis=0).fillna(0)
    return w

# ---------- Indicateur d‚Äô√©tat de donn√©es (affich√© en sidebar) ----------
DATA_STATUS: "OrderedDict[str, dict]" = OrderedDict()

def set_status(name: str, ok: bool, detail: str = ""):
    DATA_STATUS[name] = {"ok": bool(ok), "detail": str(detail or "")}

def call_safely(_fn, *args, label: str = None, **kwargs):
    """Ex√©cute une fonction; enregistre OK/√©chec dans DATA_STATUS (affich√© en UI)."""
    try:
        out = _fn(*args, **kwargs)
        if label:
            ok = out is not None and (not hasattr(out, "empty") or not getattr(out, "empty", False))
            set_status(label, ok, "ok" if ok else "vide")
        return out
    except Exception as e:
        if label:
            set_status(label, False, str(e))
        log_warn(f"{_fn.__name__} en √©chec ({label or ''}): {e}")
        return None

def render_sources_state():
    df = get_dataset_log_latest()
    st.subheader("√âtat des sources (derni√®re collecte)")
    st.dataframe(df, width='stretch')

def badge(available: bool, name: str, fallback: str = ""):
    """Affiche un badge indiquant l'√©tat d'une source de donn√©es"""
    if available:
        st.success(f"‚úÖ {name}")
    else:
        st.error(f"‚ùå {name} - {fallback}" if fallback else f"‚ùå {name}")

# ================== CONNECTEURS EXTERNES ==================
log = logging.getLogger("macroapp")

@with_span("fetch_gscpi")
def fetch_gscpi():
    """
    Charge GSCPI depuis plusieurs miroirs.
    G√®re les lignes d'en-t√™te/metadata et CSV non standard.
    """
    import io
    import re
    URLS = [
        # miroir Fed (HTML/CSV), attention: peut servir du CSV "sale"
        "https://www.newyorkfed.org/medialibrary/research/gscpi/files/gscpi_data.csv",
        # miroir communautaire
        "https://raw.githubusercontent.com/QUANTAXIS/QAData/main/gscpi_data.csv",
        # backup quantnomad (si dispo)
        "https://raw.githubusercontent.com/QuantNomad/public-datasets/main/gscpi/gscpi_data.csv",
    ]
    last_err = None
    for url in URLS:
        try:
            r = SESSION.get(url, timeout=10)
            r.raise_for_status()
            raw = r.text

            # Nettoyage basique: enlever BOM, lignes vides, heading non-CSV
            raw = raw.lstrip("\ufeff")
            # Certaines versions contiennent des lignes de commentaire/texte
            lines = [ln for ln in raw.splitlines() if ln.strip()]
            # Garder √† partir de la 1√®re ligne qui contient une virgule ou un ';'
            start = 0
            for i, ln in enumerate(lines):
                if ("," in ln) or (";" in ln):
                    start = i
                    break
            cleaned = "\n".join(lines[start:])

            # Essais de parsing: coma puis point-virgule
            for sep in (",", ";"):
                try:
                    df = pd.read_csv(io.StringIO(cleaned), sep=sep)
                    break
                except Exception:
                    df = None
            if df is None or df.empty:
                raise ValueError("CSV vide / illisible")

            # Normaliser colonnes
            cols_l = {c.lower(): c for c in df.columns}
            dcol = cols_l.get("date")
            if not dcol:
                # certaines variantes utilisent 'observation_date'
                dcol = cols_l.get("observation_date")
            if not dcol:
                raise ValueError("colonne date manquante")

            gcol = None
            for c in df.columns:
                if re.sub(r"[^a-z]", "", c.lower()) in ("gscpi", "global_supply_chain_pressureindex"):
                    gcol = c
                    break
            if not gcol:
                # parfois colonne 1
                gcol = df.columns[1] if len(df.columns) > 1 else None
            if not gcol:
                raise ValueError("colonne GSCPI manquante")

            df = df[[dcol, gcol]].rename(columns={dcol: "date", gcol: "GSCPI"})
            df["date"] = pd.to_datetime(df["date"], errors="coerce")
            df = df.dropna(subset=["date"]).set_index("date").sort_index()
            df["GSCPI"] = pd.to_numeric(df["GSCPI"], errors="coerce")
            df = df.dropna(subset=["GSCPI"])

            meta = df_fingerprint(df)
            write_entry("GSCPI", url, "ok", meta, "v1")
            log.info("gscpi_ok", extra={"rows": meta["rows"], "min_date": meta["min_date"], "max_date": meta["max_date"]})
            return df

        except Exception as e:
            last_err = str(e)
            log.warning("gscpi_try_fail", extra={"url": url, "err": last_err})
            continue

    write_entry("GSCPI", ";".join(URLS), "fail", df_fingerprint(pd.DataFrame()), "v1")
    log.error("gscpi_fail", extra={"err": last_err})
    return None

@tlog("fetch_vix_history")
def fetch_vix_history():
    """Indice de volatilit√© CBOE (VIX) quotidien ‚Üí pd.Series(name='VIX')."""
    url = "https://cdn.cboe.com/api/global/us_indices/daily_prices/VIX_History.csv"
    df = pd.read_csv(url, parse_dates=["DATE"])
    return df.set_index("DATE")["CLOSE"].rename("VIX")

@with_span("fetch_gpr")
def fetch_gpr():
    """
    Indice de risque g√©opolitique (Iacoviello).
    Plusieurs miroirs + arr√™t propre si tous √©chouent.
    """
    import io
    URLS = [
        "https://www2.bc.edu/matteo-iacoviello/gpr_files/GPRD.csv",
        "https://www.matteoiacoviello.com/gpr_files/GPRD.csv",
        "https://raw.githubusercontent.com/QuantNomad/public-datasets/main/gpr/GPRD.csv",
        "https://raw.githubusercontent.com/pmorissette/gpr-data/main/GPRD.csv",
    ]
    last_err = None
    for url in URLS:
        try:
            r = SESSION.get(url, timeout=10)
            r.raise_for_status()
            df = pd.read_csv(io.StringIO(r.text))
            # normaliser
            dcol = next((c for c in df.columns if c.lower() in ("date","month")), None)
            if not dcol:
                raise ValueError("GPR: colonne date/mois manquante")
            df[dcol] = pd.to_datetime(df[dcol], errors="coerce")
            df = df.dropna(subset=[dcol]).set_index(dcol).sort_index()
            gcol = "GPR" if "GPR" in df.columns else next((c for c in df.columns if c.upper()=="GPR"), None)
            if gcol:
                df = df[[gcol]].astype(float).rename(columns={gcol:"GPR"})
            else:
                # certaines versions ont 'gpr' en lower
                gcol = next((c for c in df.columns if c.lower()=="gpr"), None)
                if gcol:
                    df = df[[gcol]].astype(float).rename(columns={gcol:"GPR"})
            if df.empty:
                raise ValueError("GPR vide")
            meta = df_fingerprint(df)
            write_entry("GPR", url, "ok", meta, "v1")
            log.info("gpr_ok", extra={"rows": meta["rows"], "min_date": meta["min_date"], "max_date": meta["max_date"]})
            return df
        except Exception as e:
            last_err = str(e)
            log.warning("gpr_try_fail", extra={"url": url, "err": last_err})
            continue

    write_entry("GPR", ";".join(URLS), "fail", df_fingerprint(pd.DataFrame()), "v1")
    log.error("gpr_fail", extra={"err": last_err})
    return None

@tlog("fetch_boc_fx")
def fetch_boc_fx(series="FXUSDCAD"):
    """Taux de change Banque du Canada (USD/CAD)."""
    url = f"https://www.bankofcanada.ca/valet/observations/{series}?start_date=2010-01-01"
    r = requests.get(url, timeout=30); r.raise_for_status()
    data = r.json()["observations"]
    df = pd.DataFrame(data)
    df["d"] = pd.to_datetime(df["d"])
    df[series] = pd.to_numeric(df[series], errors="coerce")
    return df.set_index("d")[series]

def fetch_eia(series_id, api_key):
    """S√©ries EIA (√©nergie)."""
    url = f"https://api.eia.gov/series/?api_key={api_key}&series_id={series_id}"
    r = requests.get(url, timeout=30); r.raise_for_status()
    series = r.json()["series"][0]["data"]
    df = pd.DataFrame(series, columns=["date","value"])
    df["date"] = pd.to_datetime(df["date"], format="%Y%m%d", errors="coerce").fillna(
                 pd.to_datetime(df["date"], format="%Y%m", errors="coerce"))
    return df.set_index("date")["value"].astype(float).sort_index()

def fetch_bls(series_ids, api_key=None, start_year=2010):
    """Bureau of Labor Statistics (ex: s√©ries d‚Äôemploi, salaires‚Ä¶)."""
    url = "https://api.bls.gov/publicAPI/v2/timeseries/data/"
    payload = {"seriesid": series_ids,
               "startyear": str(start_year),
               "endyear": str(datetime.today().year)}
    if api_key:
        payload["registrationkey"] = api_key
    r = requests.post(url, json=payload, timeout=30); r.raise_for_status()
    out = {}
    for s in r.json().get("Results",{}).get("series",[]):
        sid = s["seriesID"]; rows = []
        for item in s["data"]:
            y = item["year"]; p = item["period"]
            if p.startswith("M"):
                month = int(p[1:])
                rows.append([pd.to_datetime(f"{y}-{month:02d}-01"), float(item["value"])])
        out[sid] = pd.Series(dict(rows)).sort_index()
    return pd.DataFrame(out)

def fetch_gdelt_events(days=30):
    """Proxy de chocs ‚Äòtarifs/guerre‚Äô bas√© sur le volume d‚Äô√©v√©nements GDELT."""
    try:
        frames = []
        for d in range(days):
            day = (datetime.now(timezone.utc) - timedelta(days=d+1)).strftime("%Y%m%d")
            url = f"http://data.gdeltproject.org/gdeltv2/{day}000000.summary.gz"
            df = pd.read_csv(url, compression="gzip", sep="\t", header=None, low_memory=False)
            df["date"] = pd.to_datetime(day)
            frames.append(df[["date"]])
            time.sleep(0.1)
        all_df = pd.concat(frames, ignore_index=True)
        s = all_df.groupby("date").size()
        return s.sort_index()
    except Exception:
        return pd.Series(dtype=float)

# ===== Fuseau horaire New York pour le calendrier =====
try:
    import pytz
    US_TZ = pytz.timezone("America/New_York")
except Exception:
    US_TZ = None

def today_ny():
    if US_TZ:
        return datetime.now(US_TZ).date()
    return datetime.now(timezone.utc).date()

@tlog("fetch_te_calendar")
@st.cache_data(ttl=60*15, show_spinner=False)
def fetch_te_calendar(d1: str, d2: str, country="United States", client="guest:guest"):
    """Calendrier des publications √©conomiques (TradingEconomics)"""
    base = "https://api.tradingeconomics.com/calendar"
    params = {"d1": d1, "d2": d2, "c": country, "format": "json", "client": client}
    try:
        r = requests.get(base, params=params, timeout=20)
        r.raise_for_status()
        df = pd.DataFrame(r.json())
        if df.empty:
            return df
        keep = ["Country","Category","Event","Reference","DateUtc","TimeUtc",
                "Importance","Actual","Previous","Forecast"]
        for k in keep:
            if k not in df.columns:
                df[k] = None
        df["ts_utc"] = pd.to_datetime(df["DateUtc"] + " " + df["TimeUtc"], errors="coerce", utc=True)
        if US_TZ is not None:
            df["ts_ny"] = df["ts_utc"].dt.tz_convert(US_TZ)
        else:
            df["ts_ny"] = df["ts_utc"]
        imp_map = {"Low":1, "Medium":2, "High":3}
        df["imp"] = df["Importance"].map(imp_map).fillna(0)
        df = df.sort_values(["imp","ts_ny"], ascending=[False, True])
        return df[keep + ["ts_ny","imp"]]
    except Exception:
        return pd.DataFrame()

# ================== MOD√àLES / SCORES ==================
def compute_theme_scores(fred_df, params):
    """Calcule des scores mensuels par grands th√®mes macro, normalis√©s [-1..1]."""
    m = fred_df.copy()
    m = m.resample("D").last().ffill().resample("ME").last()

    cpi_yoy      = m["CPIAUCSL"].pct_change(12, fill_method=None)*100 if "CPIAUCSL" in m else pd.Series(index=m.index)
    infl_exp     = m.get("T10YIE", pd.Series(index=m.index))
    indpro_yoy   = m["INDPRO"].pct_change(12, fill_method=None)*100 if "INDPRO" in m else pd.Series(index=m.index)
    unrate       = m.get("UNRATE", pd.Series(index=m.index))
    payrolls_mom = m["PAYEMS"].pct_change(1, fill_method=None)*100 if "PAYEMS" in m else pd.Series(index=m.index)
    dgs10        = m.get("DGS10", pd.Series(index=m.index))
    dgs2         = m.get("DGS2",  pd.Series(index=m.index))
    curve        = (dgs10 - dgs2) if not dgs10.empty and not dgs2.empty else pd.Series(index=m.index)
    usd          = m.get("DTWEXBGS", pd.Series(index=m.index))

    z_infl      = zscore(cpi_yoy,      params["win_infl"]).clip(-3,3)
    z_infl_exp  = zscore(infl_exp,     params["win_infl"]).clip(-3,3)
    z_growth    = zscore(indpro_yoy,   params["win_growth"]).clip(-3,3)
    z_jobs      = (-zscore(unrate,     params["win_jobs"])).clip(-3,3)
    z_payrolls  = zscore(payrolls_mom, params["win_jobs"]).clip(-3,3)
    z_rates     = zscore(dgs10,        params["win_rates"]).clip(-3,3)
    z_curve     = zscore(curve,        params["win_rates"]).clip(-3,3)
    z_usd       = zscore(usd,          params["win_usd"]).clip(-3,3)

    squash = lambda x: np.tanh(x/2.0)
    theme = pd.DataFrame(index=m.index)
    theme["Inflation"]           = squash(0.6*z_infl + 0.4*z_infl_exp)
    theme["Croissance"]          = squash(0.7*z_growth + 0.3*z_payrolls)
    theme["March√© du travail"]   = squash(z_jobs)
    theme["Taux d‚Äôint√©r√™t"]      = squash(0.7*z_rates - 0.3*z_curve)
    theme["Dollar am√©ricain"]    = squash(z_usd)
    return theme.fillna(0)

def sector_scores(theme_df, sensitivities):
    """Score des secteurs = combinaison lin√©aire (th√®mes √ó sensibilit√©s), puis mise √† l‚Äô√©chelle robuste [-5..5]."""
    raw = theme_df.dot(sensitivities.T)
    raw = raw.where(raw.notna().any(axis=1), other=0)
    scaled = raw.apply(robust_minmax, axis=1)
    return scaled, raw

def backtest_rotation(prices, scores_scaled, top_k=3, ema=3, margin=0.15):
    """Strat√©gie mensuelle : prendre les K meilleurs secteurs (avec lissage et hyst√©r√©sis)."""
    monthly_prices = prices.resample("ME").last().dropna(how="all")
    monthly_scores = scores_scaled.resample("ME").last().reindex(monthly_prices.index).dropna(how="all")
    monthly_scores = smooth_scores(monthly_scores, ema=ema)
    cols = monthly_scores.columns.intersection(monthly_prices.columns)
    monthly_prices = monthly_prices[cols]
    monthly_scores = monthly_scores[cols]
    w = hysteresis_picks(monthly_scores.shift(1), k=top_k, margin=margin)
    rets = monthly_prices.pct_change().fillna(0.0)
    strat_ret = (w * rets).sum(axis=1)
    nav = (1 + strat_ret).cumprod()
    return nav, strat_ret, w

# ======================================================================================
# RENDU PRINCIPAL (TOUT est ex√©cut√© ici, rien √† l‚Äôimport)
# ======================================================================================
def render_macro():
    # ---- Trace ID visible UI
    if "trace_id" not in st.session_state or not st.session_state["trace_id"]:
        st.session_state["trace_id"] = new_trace_id()
    else:
        set_trace_id(st.session_state["trace_id"])
    st.caption(f"Trace ID: `{st.session_state['trace_id']}`")

    # En-t√™te
    st.header("Pr√©vision macro√©conomique et rotation sectorielle (affichage complet)")
    st.caption("Tous les √©l√©ments sont affich√©s sans abr√©viations ambigu√´s. Les intitul√©s pr√©cisent ce que montre chaque graphique ou tableau.")

    # -------------------- R√âGLAGES (toujours visibles) --------------------
    st.subheader("R√©glages de l‚Äôanalyse")
    colA, colB, colC = st.columns([1.2, 1, 1])
    with colA:
        # Cl√© FRED (si disponible en env / secrets)
        try:
            from secrets_local import get_key  # type: ignore
            fred_default = get_key("FRED_API_KEY") or os.environ.get("FRED_API_KEY", "")
        except Exception:
            fred_default = os.environ.get("FRED_API_KEY", "")
        fred_key = st.text_input("Cl√© API FRED (facultatif)", value=fred_default, help="Laissez vide si vous n‚Äôen avez pas. On basculera sur une m√©thode alternative.")
    with colB:
        start_date = st.date_input("Date de d√©but des s√©ries macro", value=datetime(2010,1,1))
    with colC:
        preset = st.selectbox("Profil d‚Äôanalyse (fen√™tres de calcul)", ["Standard","Banque centrale restrictive","Reflation","Aversion au risque"])

    col1, col2, col3 = st.columns(3)
    with col1:
        top_k     = st.slider("Nombre de secteurs retenus simultan√©ment", 1, 6, 3, help="K meilleurs secteurs retenus chaque mois.")
    with col2:
        ema_span  = st.slider("Lissage exponentiel (nombre de mois)", 1, 12, 3, help="Plus la valeur est √©lev√©e, plus les scores sont liss√©s.")
    with col3:
        hyst_mag  = st.slider("Marge d‚Äôhyst√©r√©sis (stabilit√© de s√©lection)", 0.0, 0.5, 0.15, 0.05, help="√âvite de changer trop souvent de secteurs proches du seuil.")

    st.markdown("---")

    # Sensibilit√©s
    st.subheader("Sensibilit√©s des secteurs aux th√®mes macro√©conomiques")
    include_gdx = st.checkbox("Inclure le secteur 'mines d‚Äôor' (ETF GDX)", value=True)
    sens_scale = st.slider("√âchelle globale de sensibilit√©", 0.5, 3.0, 1.0, 0.1,
                           help="Ajuste l‚Äôintensit√© des sensibilit√©s entre th√®mes et secteurs.")
    symbols = SECTOR_ETFS + (EXTRA_ETFS if include_gdx else [])
    editable = DEFAULT_SENS.loc[symbols, ["Inflation","Croissance","Taux d‚Äôint√©r√™t","Dollar am√©ricain","March√© du travail"]].copy()
    st.caption("Modifie si besoin la matrice de sensibilit√©s (positif = th√®me favorable au secteur).")
    grid_cols = st.columns(len(editable.columns))
    for j, th in enumerate(editable.columns):
        with grid_cols[j]:
            st.markdown(f"**{th}**")
            for sec in editable.index:
                editable.loc[sec, th] = st.number_input(f"{sec} ‚Üî {th}", value=float(editable.loc[sec, th]), step=0.1, key=f"{sec}_{th}")
    SENS_USED = editable * sens_scale

    st.markdown("---")

    # Facteurs de risque externes
    st.subheader("Facteurs de risque externes √† int√©grer aux scores")
    colR1, colR2, colR3 = st.columns(3)
    with colR1:
        use_exo = st.checkbox("Ajuster par risques externes (volatilit√©, g√©opolitique, cha√Ænes logistiques)", value=True,
                              help="Utilise VIX (volatilit√©), GPR (risque g√©opolitique), GSCPI (pressions d‚Äôapprovisionnement).")
    with colR2:
        use_cad = st.checkbox("Ajuster par taux de change dollar US / dollar canadien (USD/CAD)", value=True,
                              help="Impact sur les mati√®res premi√®res.")
    with colR3:
        use_tarif = st.checkbox("Ajuster par proxy 'tarifs/guerre' (GDELT)", value=False,
                                help="Bas√© sur le volume d‚Äô√©v√©nements GDELT.")

    # Calendrier macro
    st.subheader("Param√®tre du calendrier des publications √©conomiques")
    te_client  = st.text_input("Identifiants TradingEconomics (utilisateur:cl√©) ‚Äî facultatif", value=os.environ.get("TE_CLIENT","guest:guest"))
    show_cal = st.checkbox("Afficher le calendrier √©conomique des √âtats-Unis (hier, aujourd‚Äôhui, demain)", value=True)

    st.markdown("---")

    # Bouton de rechargement
    if st.button("Recharger toutes les donn√©es"):
        st.cache_data.clear()
        st.rerun()

    # -------------------- PARAM√àTRES D‚ÄôANALYSE --------------------
    params = {"win_infl": 24, "win_growth": 24, "win_jobs": 24, "win_rates": 24, "win_usd": 24}
    if preset == "Banque centrale restrictive":
        params.update(win_infl=18, win_rates=18, win_usd=18)
    elif preset == "Reflation":
        params.update(win_growth=12, win_jobs=12, win_infl=12)
    elif preset == "Aversion au risque":
        params.update(win_rates=36, win_usd=36)

    st.caption("Les ‚Äòsurprises‚Äô sont estim√©es via des z-scores roulants sur des fen√™tres exprim√©es en mois.")

    # -------------------- CHARGEMENT DES DONN√âES --------------------
    @st.cache_data(ttl=3600, show_spinner=False)
    def cached_fetch_macro(fred_key, start):
        frames = []
        for sid in FRED_SERIES.keys():
            df = load_fred_series(sid, fred_key, start=start)
            if df is None or df.empty:
                continue
            frames.append(df)
            time.sleep(0.05)
        if not frames:
            return pd.DataFrame()
        all_df = pd.concat(frames, axis=1, join="outer").sort_index()
        all_df = all_df[all_df.index >= pd.to_datetime(start)]
        return all_df

    macro = cached_fetch_macro(fred_key, start_date)
    set_status("Donn√©es macro√©conomiques (FRED)", not macro.empty, "ok" if not macro.empty else "vide")
    if macro.empty:
        st.error("Impossible de charger les s√©ries macro√©conomiques. V√©rifie ta connexion ou la cl√© FRED.")
        st.stop()

    themes = compute_theme_scores(macro, params)
    if getattr(themes.index, "tz", None) is not None:
        themes.index = themes.index.tz_localize(None)

    scores_scaled, scores_raw = sector_scores(themes, SENS_USED)

    @st.cache_data(ttl=3600, show_spinner=False)
    def cached_fetch_prices(symbols, start):
        return get_multi_yf(symbols, start=str(start))

    prices = cached_fetch_prices(symbols, start_date)
    if getattr(prices.index, "tz", None) is not None:
        prices.index = prices.index.tz_localize(None)
    align_start = pd.to_datetime(themes.index.min())
    prices = prices[prices.index >= align_start]
    set_status("Prix des secteurs (Yahoo Finance)", not prices.empty, "ok" if not prices.empty else "vide")

    # -------------------- AJUSTEMENTS EXOG√àNES --------------------
    scores_scaled_exo = scores_scaled.copy()
    overlay_notes = []

    if use_exo:
        gscpi = call_safely(fetch_gscpi, label="Indice de pression des cha√Ænes d‚Äôapprovisionnement (GSCPI)")
        vix   = call_safely(fetch_vix_history, label="Indice de volatilit√© CBOE (VIX)")
        gpr   = call_safely(fetch_gpr, label="Indice de risque g√©opolitique (GPR)")
        exo = []
        if gscpi is not None: exo.append(gscpi.rename("GSCPI").resample("ME").last())
        if vix   is not None: exo.append(vix.rename("VIX").resample("ME").last())
        if gpr   is not None: exo.append(gpr.rename("GPR").resample("ME").last())
        if exo:
            exo = pd.concat(exo, axis=1).reindex(scores_scaled_exo.index)
            exo_z = exo.apply(lambda s: (s - s.rolling(24).mean())/s.rolling(24).std())
            risk_penalty = pd.Series(0, index=scores_scaled_exo.index, dtype=float)
            if "VIX" in exo_z:   risk_penalty = risk_penalty.add((-0.15)*exo_z["VIX"].fillna(0), fill_value=0)
            if "GPR" in exo_z:   risk_penalty = risk_penalty.add((-0.15)*exo_z["GPR"].fillna(0), fill_value=0)
            if "GSCPI" in exo_z: risk_penalty = risk_penalty.add((-0.10)*exo_z["GSCPI"].fillna(0), fill_value=0)
            scores_scaled_exo = scores_scaled_exo.add(risk_penalty, axis=0).clip(-5,5)
            try:
                overlay_notes.append(f"Ajustement ‚Äòrisques externes‚Äô appliqu√© (somme la plus r√©cente : {risk_penalty.iloc[-1]:+.2f})")
            except Exception:
                pass

    render_sources_state()

    if gscpi is None:
        st.error(f"‚ùå GSCPI indisponible ‚Äî voir '√âtat des sources‚Äô. Trace ID: `{get_trace_id()}`")
    if gpr is None:
        st.warning(f"‚ö†Ô∏è GPR indisponible ‚Äî trace `{get_trace_id()}`")

    if use_cad:
        cad = call_safely(fetch_boc_fx, series="FXUSDCAD",
                          label="Taux de change USD/CAD (Banque du Canada)")
        if cad is not None and not cad.empty:
            cad_m = cad.resample("ME").last().reindex(scores_scaled_exo.index)
            cad_z = (cad_m - cad_m.rolling(24).mean())/cad_m.rolling(24).std()
            adj = (-0.10)*cad_z.squeeze().fillna(0)  # USD‚Üë(CAD‚Üì) ‚Üí vent contraire pour mati√®res premi√®res
            scores_scaled_exo = scores_scaled_exo.add(adj, axis=0).clip(-5,5)
            try:
                overlay_notes.append(f"Ajustement USD/CAD : {adj.iloc[-1]:+.2f} (valeur la plus r√©cente)")
            except Exception:
                pass

    if use_tarif:
        gd = call_safely(fetch_gdelt_events, days=60, label="Proxy ‚Äòtarifs/guerre‚Äô (GDELT)")
        if gd is not None and len(gd)>0:
            tw = gd.resample("ME").sum()
            tw_z = (tw - tw.rolling(12).mean())/tw.rolling(12).std()
            pen = (-0.10)*tw_z.reindex(scores_scaled_exo.index).fillna(0)
            scores_scaled_exo = scores_scaled_exo.add(pen, axis=0).clip(-5,5)
            try:
                overlay_notes.append(f"Proxy ‚Äòtarifs/guerre‚Äô : {pen.iloc[-1]:+.2f} (valeur la plus r√©cente)")
            except Exception:
                pass

    # -------------------- CALENDRIER DES PUBLICATIONS --------------------
    if show_cal:
        st.subheader("Calendrier √©conomique des √âtats-Unis (hier, aujourd‚Äôhui, demain)")
        d_today = today_ny()
        d1 = (d_today - timedelta(days=1)).strftime("%Y-%m-%d")
        d2 = (d_today + timedelta(days=1)).strftime("%Y-%m-%d")
        cal = fetch_te_calendar(d1, d2, client=te_client)
        set_status("Calendrier des publications (TradingEconomics)", not cal.empty, "ok" if not cal.empty else "vide")

        if cal.empty:
            st.info("Aucun √©v√©nement sur la fen√™tre choisie ou service temporairement indisponible.")
        else:
            cal["Heure (New York)"] = cal["ts_ny"].dt.strftime("%a %m-%d %H:%M")
            cal_view = (cal[["Heure (New York)","Event","Category","Reference","Actual","Forecast","Previous","Importance"]]
                        .sort_values(["Importance","Heure (New York)"], ascending=[False, True]))
            st.dataframe(cal_view, width='stretch', height=280)

            # Focus √©v√©nements importants √† venir aujourd‚Äôhui
            try:
                d_today_start = pd.Timestamp(d_today, tz=US_TZ) if US_TZ is not None else pd.Timestamp(d_today)
                top = cal[(cal["imp"] >= 2) & (cal["ts_ny"] >= d_today_start)].head(6)
                if not top.empty:
                    st.markdown("**√Ä surveiller (importance moyenne/√©lev√©e, heures de New York)**")
                    for _, r in top.iterrows():
                        imp_badge = "üî¥" if r["Importance"] == "High" else "üü†"
                        st.write(
                            f"{imp_badge} {r['ts_ny']:%a %H:%M} ‚Äî {r['Event']} "
                            f"(Actuel : {r['Actual']}, Consensus : {r['Forecast']}, Pr√©c√©dent : {r['Previous']})"
                        )
            except Exception as e:
                log_warn(f"Top events cal √©chou√©: {e}")

    st.markdown("---")

    # -------------------- √âTAT DES SOURCES DE DONN√âES --------------------
    st.subheader("√âtat des sources de donn√©es (chargements externes)")
    if not DATA_STATUS:
        st.caption("Aucun chargement externe enregistr√© pour l'instant.")
    else:
        rows = []
        for name, info in DATA_STATUS.items():
            emoji = "‚úÖ" if info["ok"] else "‚ùå"
            detail = f" ‚Äî {info['detail']}" if info.get("detail") else ""
            rows.append(f"{emoji} **{name}**{detail}")
        st.markdown("\n".join(f"- {r}" for r in rows))

    st.markdown("---")

    # Add source status badges
    badge(gscpi is not None and not gscpi.empty if 'gscpi' in locals() else False,
          "Indice de pression des cha√Ænes d'approvisionnement (GSCPI)",
          "source indisponible, remplac√© par placeholder" if gscpi is None or gscpi.empty else "")

    badge(gpr is not None and not gpr.empty if 'gpr' in locals() else False,
          "Indice de risque g√©opolitique (GPR)",
          "source indisponible, saute automatiquement" if gpr is None or gpr.empty else "")

    # -------------------- DIAGNOSTICS ET LOGS --------------------
    st.subheader("Diagnostics (couverture, profils) et journal d‚Äôex√©cution")
    c1, c2, c3, c4 = st.columns(4)
    c1.metric("Nombre de lignes ‚Äî s√©ries FRED", macro.shape[0] if not macro.empty else 0)
    c2.metric("Nombre de lignes ‚Äî th√®mes macro", themes.shape[0] if not themes.empty else 0)
    c3.metric("Nombre de lignes ‚Äî prix des secteurs", prices.shape[0] if not prices.empty else 0)
    c4.metric("Nombre de secteurs analys√©s", prices.shape[1] if not prices.empty else 0)

    prof = []
    prof.append(profile_df(macro, "S√©ries FRED (macro)"))
    prof.append(profile_df(themes, "Th√®mes macro calcul√©s"))
    prof.append(profile_df(prices, "Prix des secteurs (yfinance)"))
    try:
        # ‚Äòexo‚Äô peut exister si use_exo True
        if "exo" in locals():
            prof.append(profile_df(exo, "Facteurs externes (VIX, GPR, GSCPI)"))
    except Exception:
        pass
    st.markdown("**Profils des tableaux de donn√©es**")
    st.dataframe(pd.DataFrame(prof), width='stretch', height=240)

    if not macro.empty:
        st.markdown("**Derni√®re date non-nulle par s√©rie FRED**")
        last_valid = {c: (macro[c].last_valid_index() if macro[c].notna().any() else None) for c in macro.columns}
        st.dataframe(pd.DataFrame.from_dict(last_valid, orient="index", columns=["Derni√®re date non nulle"]).sort_index(),
                     width='stretch', height=240)

    # Journal (dernier millier de lignes)
    st.markdown("**Journal des op√©rations (extraits r√©cents)**")
    logs = "\n".join(st.session_state.logbuf[-1000:])
    st.code(logs or "Aucun log pour l‚Äôinstant.", language="text")

    st.markdown("---")

    # -------------------- INDICATEURS DE TH√àMES (derni√®re valeur) --------------------
    st.subheader("Indicateurs de th√®mes macro√©conomiques ‚Äî derni√®re valeur (√©chelle ‚àí100 √† +100)")
    latest = themes.dropna().iloc[-1] if not themes.dropna().empty else pd.Series(dtype=float)
    order = ["Croissance","Inflation","Taux d‚Äôint√©r√™t","Dollar am√©ricain","March√© du travail"]
    color_map = {
        "Croissance":        "#22c55e",
        "Inflation":         "#ef4444",
        "Taux d‚Äôint√©r√™t":    "#3b82f6",
        "Dollar am√©ricain":  "#8b5cf6",
        "March√© du travail": "#f59e0b",
    }
    rows = [order[:3], order[3:]]
    for row in rows:
        cols = st.columns(len(row))
        for c, name in zip(cols, row):
            with c:
                val = float(latest.get(name, 0.0))
                fig = go.Figure(go.Indicator(
                    mode="gauge+number+delta",
                    value=val*100, delta={'reference': 0},
                    title={'text': name},
                    gauge={'axis': {'range': [-100, 100]},
                           'bar': {'color': color_map.get(name, "#10b981")}}
                ))
                fig.update_layout(height=170, margin=dict(l=8,r=8,t=30,b=8))
                st.plotly_chart(fig, width='stretch')

    st.markdown("---")

    # -------------------- GRANDS GRAPHIQUES --------------------
    st.subheader("Scores des secteurs (12 derniers mois) ‚Äî apr√®s normalisation et ajustements externes")
    last12 = scores_scaled_exo.tail(12)
    fig = go.Figure()
    for c in last12.columns:
        diffs = last12[c].diff().fillna(0).values
        fig.add_trace(go.Scatter(
            x=last12.index, y=last12[c], name=c,
            customdata=diffs,
            hovertemplate="Date: %{x|%Y-%m-%d}<br>Score normalis√©: %{y:.2f}<br>Variation mensuelle: %{customdata:+.2f}<extra>"+c+"</extra>"
        ))
    fig.update_layout(height=360, yaxis_title="Score normalis√© (de ‚àí5 √† +5)")
    st.plotly_chart(fig, width='stretch')

    # Simulation historique (backtest)
    st.subheader("Simulation historique (backtest) ‚Äî strat√©gie mensuelle Top-K avec lissage et hyst√©r√©sis")
    nav, strat_ret, weights = backtest_rotation(
        prices, scores_scaled_exo, top_k=top_k, ema=ema_span, margin=hyst_mag
    )
    use_log = st.checkbox("Afficher l‚Äô√©chelle logarithmique pour la courbe de performance", value=False, key="log_nav")
    fig2 = go.Figure()
    fig2.add_trace(go.Scatter(x=nav.index, y=nav, name="Strat√©gie (valeur cumul√©e, normalis√©e au d√©part)"))
    try:
        spy = get_asset_price("SPY", start=str(start_date))
        spy_nav = (1 + spy.pct_change(fill_method=None).fillna(0)).cumprod()
        fig2.add_trace(go.Scatter(x=spy_nav.index, y=spy_nav/spy_nav.iloc[0], name="Indice S&P 500 (SPY) ‚Äî normalis√©"))
    except Exception as e:
        log_warn(f"R√©cup√©ration SPY impossible: {e}")
    try:
        ew = (1 + prices.pct_change().mean(axis=1)).cumprod()
        fig2.add_trace(go.Scatter(x=ew.index, y=ew/ew.iloc[0], name="√âgal-pond√©ration des secteurs ‚Äî normalis√©e"))
    except Exception as e:
        log_warn(f"Calcul √©gal-pond√©ration impossible: {e}")

    # Ombrage des p√©riodes de r√©cession (NBER)
    try:
        usrec = macro.get("USREC")
        if usrec is None or (hasattr(usrec, "empty") and usrec.empty):
            usrec = load_fred_series("USREC", fred_key)
        usrec_m = usrec.resample("ME").last().reindex(nav.index).fillna(0)
        in_rec = False; rec_start = None
        for i in range(len(usrec_m)):
            flag = usrec_m.iloc[i,0] if isinstance(usrec_m, pd.DataFrame) else usrec_m.iloc[i]
            if not in_rec and flag == 1:
                in_rec = True; rec_start = usrec_m.index[i]
            if in_rec and flag == 0:
                in_rec = False
                fig2.add_vrect(x0=rec_start, x1=usrec_m.index[i], fillcolor="grey", opacity=0.09, line_width=0)
        if in_rec:
            fig2.add_vrect(x0=rec_start, x1=usrec_m.index[-1], fillcolor="grey", opacity=0.09, line_width=0)
    except Exception as e:
        log_warn(f"Ombres r√©cession NBER ignor√©es: {e}")

    fig2.update_layout(height=440, yaxis_title="Valeur cumul√©e (normalis√©e au d√©part)",
                       yaxis_type=("log" if use_log else "linear"))
    st.plotly_chart(fig2, width='stretch')

    # Carte de chaleur
    st.subheader("Carte de chaleur des scores sectoriels (12 derniers mois)")
    hm = scores_scaled_exo.tail(12)
    if not hm.empty:
        fig = go.Figure(data=go.Heatmap(
            z=hm.values, x=hm.columns, y=[d.strftime("%Y-%m") for d in hm.index],
            zmin=-5, zmax=5, colorscale="RdBu",
            colorbar=dict(orientation="h", y=-0.2, title="Score normalis√© (de ‚àí5 √† +5)")
        ))
        fig.update_layout(height=380, margin=dict(b=60))
        st.plotly_chart(fig, width='stretch')

    st.markdown("---")

    # -------------------- CLASSEMENT ACTUEL --------------------
    st.subheader("Classement actuel des secteurs (avec variation depuis le mois pr√©c√©dent)")
    if not scores_scaled_exo.dropna().empty:
        idxs = scores_scaled_exo.dropna().index
        idxs_sorted = idxs.sort_values()
        last_idx = idxs_sorted[-1]
        prev_idx = idxs_sorted[-2] if len(idxs_sorted) > 1 else None

        cur = scores_scaled_exo.loc[last_idx].sort_values(ascending=False).to_frame("Score normalis√©")
        if prev_idx is not None:
            prev = scores_scaled_exo.loc[prev_idx].reindex(cur.index)
            delta = (cur["Score normalis√©"] - prev).rename("Variation mensuelle (Œî)")
        else:
            delta = pd.Series(0, index=cur.index, name="Variation mensuelle (Œî)")
        view = pd.concat([cur, delta], axis=1)
        view["Direction"] = np.where(view["Variation mensuelle (Œî)"]>=0, "Hausse (‚Üë)", "Baisse (‚Üì)")

        idx_top = view["Score normalis√©"].idxmax()
        def highlight_top(row):
            return ['font-weight:bold;background-color:rgba(16,185,129,.15)' if row.name==idx_top else '' for _ in row]

        styled = (view.style
                  .format({"Score normalis√©":"{:.2f}", "Variation mensuelle (Œî)":"{:+.2f}"})
                  .bar(subset=["Score normalis√©"], align="mid", color='#22c55e')
                  .apply(highlight_top, axis=1))

        st.dataframe(styled, width='stretch')
    else:
        st.info("Scores indisponibles pour l‚Äôinstant.")

    # Attribution par th√®me (brut, avant mise √† l‚Äô√©chelle)
    st.subheader("Contribution brute par th√®me (avant mise √† l‚Äô√©chelle) ‚Äî 6 derniers mois")
    st.dataframe(scores_raw.tail(6).rename_axis("Mois").style.format("{:+.2f}"), width='stretch')

    st.markdown("---")

    # -------------------- CORR√âLATIONS ROULANTES --------------------
    st.subheader("Corr√©lations roulantes (fen√™tre de 6 mois) entre un secteur et les th√®mes macro")
    pxm = prices.resample("ME").last()
    candidates = [s for s in (SECTOR_ETFS + (EXTRA_ETFS if include_gdx else [])) if s in pxm.columns]
    if candidates:
        pick_sec = st.selectbox("Choisir un secteur cot√© (ETF)", candidates, index=min(4, len(candidates)-1))
        r = pxm[pick_sec].pct_change()
        dfc = pd.concat([r.rename("Rendement du secteur"), themes], axis=1).dropna()
        if not dfc.empty:
            roll = dfc.rolling(6).corr().xs("Rendement du secteur", level=1, drop_level=False)
            roll = roll.drop(columns=["Rendement du secteur"], errors="ignore")
            fig = go.Figure()
            for c in ["Croissance","Inflation","Taux d‚Äôint√©r√™t","Dollar am√©ricain","March√© du travail"]:
                if c in roll.columns:
                    fig.add_trace(go.Scatter(x=roll.index, y=roll[c], name=c))
            fig.update_layout(height=340, yaxis_title="Corr√©lation (fen√™tre de 6 mois)")
            st.plotly_chart(fig, width='stretch')
    else:
        st.info("Pas de colonnes de prix disponibles pour calculer les corr√©lations.")

    st.markdown("---")

    # -------------------- SCREENER (HOLDINGS) --------------------
    st.subheader("Screener simple des principaux titres dans les secteurs s√©lectionn√©s")
    chosen = (cur.index[:top_k].tolist() if 'cur' in locals() and not cur.empty
              else (SECTOR_ETFS[:top_k]))
    st.caption("Secteurs analys√©s : " + ", ".join(chosen))
    for sec in chosen:
        try:
            t = yf.Ticker(sec)
            holds = getattr(t, "fund_holdings", None)
            st.markdown(f"**{sec} ‚Äî Principales lignes si disponibles**")
            if holds is None or len(holds)==0:
                st.write(f"Aucune composition d√©taill√©e disponible pour {sec}.")
                st.markdown(f"Page d‚Äôinformation : https://finance.yahoo.com/quote/{sec}")
                # Fallback : indicateurs simples sur l‚ÄôETF lui-m√™me
                try:
                    px = get_multi_yf([sec], start="2018-01-01")
                    if not px.empty:
                        m3 = px.pct_change(63).iloc[-1].rename("Performance 3 mois")
                        m6 = px.pct_change(126).iloc[-1].rename("Performance 6 mois")
                        vol = px.pct_change().rolling(63).std().iloc[-1].rename("Volatilit√© (approx.)")
                        df_fallback = pd.concat([m3, m6, vol], axis=1)
                        df_fallback["Score heuristique"] = 0.6*df_fallback["Performance 6 mois"] + 0.4*df_fallback["Performance 3 mois"] - 0.2*df_fallback["Volatilit√© (approx.)"]
                        st.dataframe(df_fallback.round(3), width='stretch')
                except Exception as e:
                    log_warn(f"Screener fallback en √©chec pour {sec}: {e}")
                continue
            if isinstance(holds, dict) and "symbol" in holds:
                tickers = pd.Series(holds["symbol"]).dropna().tolist()[:15]
            elif isinstance(holds, pd.DataFrame) and "symbol" in holds.columns:
                tickers = holds["symbol"].dropna().tolist()[:15]
            else:
                st.write(f"Format de la composition non reconnu pour {sec}.")
                continue
            px = get_multi_yf(tickers, start="2018-01-01")
            if px.empty:
                st.write(f"Aucun historique de prix pour les composants de {sec}.")
                continue
            m3 = px.pct_change(63).iloc[-1]
            m6 = px.pct_change(126).iloc[-1]
            vol = px.pct_change().rolling(63).std().iloc[-1]
            df = pd.DataFrame({
                "Performance 3 mois": m3,
                "Performance 6 mois": m6,
                "Volatilit√© (approx.)": vol
            })
            df["Score heuristique"]=0.6*df["Performance 6 mois"]+0.4*df["Performance 3 mois"]-0.2*df["Volatilit√© (approx.)"]
            st.dataframe(df.sort_values("Score heuristique", ascending=False).round(3), width='stretch')
        except Exception as e:
            st.write(f"Screener indisponible pour {sec} : {e}")

    # Notes d‚Äôajustement
    overlay_txt = " | ".join(overlay_notes) if overlay_notes else "Aucun ajustement par facteurs externes n‚Äôa √©t√© appliqu√©."
    st.info(overlay_txt)

    st.caption("Outil personnel. Ceci n‚Äôest pas un conseil d‚Äôinvestissement. Sources : FRED, yfinance, CBOE (VIX), Fed de New York (GSCPI), Banque du Canada (USD/CAD), GPR, TradingEconomics (calendrier).")
